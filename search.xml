<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Git 使用笔记</title>
      <link href="git-shi-yong/"/>
      <url>git-shi-yong/</url>
      
        <content type="html"><![CDATA[<h1 id="Git使用"><a href="#Git使用" class="headerlink" title="Git使用"></a>Git使用</h1><h1 id="GIT"><a href="#GIT" class="headerlink" title="GIT"></a>GIT</h1><ul><li>git stash ：暂存当前变更</li><li>git stash pop ： 恢复变更</li><li>git checkout 分支 ： 切换到分支</li><li>git pull ：更新到最新</li><li>git add 文件：加入追踪</li><li>git checkout -b 新分支 ： 在当前分支上创建新分支</li><li>git commit -m “–story=863210137 【紧急发布】mq_proxy优化” ：提交</li><li>git push ： 推送</li><li>git push –set-upstream origin KiHan36Month3Week1Pub_MqProxy 设置远端上游分支</li><li>git fetch origin 分支名 切换到远程分支名下</li><li>git rebase 分支： 改变基节点为[分支]的HEAD节点。</li></ul><h2 id="Rebase"><a href="#Rebase" class="headerlink" title="Rebase"></a>Rebase</h2><p>假设我们从master上签出新的分支dev1，在我们开发dev1分支的时候master上也不断有新的提交、合并分支。如果想merge 到master上的话会有新的merge记录。但是如果是用rebase就没有。因为rebase操作就是从master上重新签出一个新的临时分支，将dev1上的所有提交（从他签出master的时间开始算起的提交）都放到新的临时分支上，将冲突都解决完了之后临时分支就成为了dev1，原本dev1就被丢弃了</p><h2 id="Cherry-Pick"><a href="#Cherry-Pick" class="headerlink" title="Cherry-Pick"></a>Cherry-Pick</h2><p>其实就是复制提交，把某一个分支上的某一个提交复制到某一个分支上</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>docker 入门笔记</title>
      <link href="docker/"/>
      <url>docker/</url>
      
        <content type="html"><![CDATA[<h1 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h1><h1 id="Docker使用"><a href="#Docker使用" class="headerlink" title="Docker使用"></a>Docker使用</h1><p>概念： docker就是一个容器，可以类比于Windows上的虚拟机， 别人把image都弄好了传到社区里我们只要pull下来进入容器就可以用了。</p><p>显示可用的容器</p><pre class="line-numbers language-none"><code class="language-none">docker images<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>删除指定镜像</p><pre class="line-numbers language-none"><code class="language-none">docker rmi &lt;镜像Id&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>下载镜像</p><pre class="line-numbers language-none"><code class="language-none">docker pull hello-world<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>不指定版本号默认拉取latest版本的</p><p>删除指定镜像</p><pre class="line-numbers language-none"><code class="language-none">docker rmi &lt;镜像Id&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查看容器</p><pre class="line-numbers language-none"><code class="language-none">docker ps [OPTIONS]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>列出当前正在运行的容器, 结果的第一列是container_Id, 第2列是容器名称.</p><p>参数：</p><ul><li>a :显示所有的容器，包括未运行的。</li><li>f :根据条件过滤显示的内容。</li><li>–format :指定返回值的模板文件。</li><li>l :显示最近创建的容器。</li><li>n :列出最近创建的n个容器。</li><li>–no-trunc :不截断输出。</li><li>q :静默模式，只显示容器编号。</li><li>s :显示总的文件大小。</li></ul><p>停止指定的容器</p><pre class="line-numbers language-none"><code class="language-none">docker stop container_id/container-name 该容器Id或名称可以从docker ps中获取.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>启动容器</p><pre class="line-numbers language-none"><code class="language-none">docker start container_id/container-name 该容器Id或名称可以从docker ps中获取.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>重启容器</p><pre class="line-numbers language-none"><code class="language-none">docker restart container_id/container-name 该容器Id或名称可以从docker ps中获取.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>删除容器</p><pre class="line-numbers language-none"><code class="language-none">docker rm container_id/container-name<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>批量删除容器</p><pre class="line-numbers language-none"><code class="language-none">docker rm $(docker ps -a -q)删除所有运行结束了容器,正在运行的容器不会被删除<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>查看容器内的进程</p><pre class="line-numbers language-none"><code class="language-none">docker top container_id/container-name<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查看容器的日志输出</p><pre class="line-numbers language-none"><code class="language-none">docker logs [-f] [-t] [--tail string] 容器名, 查看容器的日志输出, -f是打开跟踪, -t是加上时间戳, --tail 100 表示仅显示最后的100行日志<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li></li><li>f : 跟踪日志输出</li><li>–since :显示某个开始时间的所有日志</li><li>t : 显示时间戳</li><li>–tail :仅列出最新N条容器日志</li></ul><p>搜寻镜像</p><pre class="line-numbers language-none"><code class="language-none">docker search镜像名字<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>显示指定镜像的详细信息</p><pre class="line-numbers language-none"><code class="language-none">docker image inspect image_iddocker container inspect container_id/container-name （包括容器的Ip）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>列出没有被容器化的镜像</p><pre class="line-numbers language-none"><code class="language-none">docker images -f dangling=true<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>删除那些没有被容器化的镜像</p><pre class="line-numbers language-none"><code class="language-none">docker rmi $(docker images -qf dangling=true)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以磁盘占用情况.</p><pre class="line-numbers language-none"><code class="language-none">docker system df<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>docker 一些管理命令集</p><p>除了上面常用的命令外, docker 还有一些管理命令集, 这些命令集还可以包含二级命令:</p><pre class="line-numbers language-none"><code class="language-none">configManageDocker configscontainerManage containersimageManage images networkManage networks node ManageSwarm nodes pluginManage plugins secret ManageDocker secrets serviceManage servicesstack Manage Docker stacksswarm Manage Swarmsystem Manage Docker trust Manage trust on Docker images volume Manage volumes<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>比较常用的是:</p><pre class="line-numbers language-none"><code class="language-none">docker image build编译Dockfiledocker network create创建 docker网络docker volume create创建数据卷<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>docker run/exec 命令</p><p>运行 hello-world 容器, 如果本地没有下载, 将会自动从hub站点下载.</p><pre class="line-numbers language-none"><code class="language-none">docker run hello-world 命令<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>以守护态运行容器</p><pre class="line-numbers language-none"><code class="language-none">docker run -d --name mybusybox busybox /bin/sh -c "while true; do echo hello world; sleep 1; done"<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>登陆一个容器, 运行中的容器其实是<strong>一个功能完备的Linux操作系统</strong>, 所以我们可以在登陆该容器执行常规的Linux命令.</p><pre class="line-numbers language-none"><code class="language-none">docker exec -it container_id/container-name /bin/bash<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>使用 redis-cli 登陆 myredis 容器</p><pre class="line-numbers language-none"><code class="language-none">docker exec -it myredis redis-cli<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>exec 后的 -it 参数的意思是, 以交互的方式并分配一个伪tty, 经常一起联用.</p><p>附录：</p><p>菜鸟教程</p><h2 id="Docker-命令大全"><a href="#Docker-命令大全" class="headerlink" title="Docker 命令大全"></a>Docker 命令大全</h2><p>容器生命周期管理</p><ul><li>run</li><li>start/stop/restart</li><li>kill</li><li>rm</li><li>pause/unpause</li><li>create</li><li>exec</li></ul><p>容器操作</p><ul><li>ps</li><li>inspect</li><li>top</li><li>attach</li><li>events</li><li>logs</li><li>wait</li><li>export</li><li>port</li></ul><p>容器rootfs命令</p><ul><li>commit</li><li>cp</li><li>diff</li></ul><p>镜像仓库</p><ul><li>login</li><li>pull</li><li>push</li><li>search</li></ul><p>本地镜像管理</p><ul><li>images</li><li>rmi</li><li>tag</li><li>build</li><li>history</li><li>save</li><li>load</li><li>import</li></ul><p>info|version</p><ul><li></li><li>info</li><li>version</li></ul><p>网络问题： 遇到<code>Error response from daemon: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network</code> 可能是网络模块有问题。</p><p>解决方案1 ： yum update</p><p>解决方案2：创建一个自己的network。<code>docker network create your-network --subnet 172.24.24.0/24</code> ，然后在yaml文件上加上</p><pre class="line-numbers language-none"><code class="language-none">version: '3'services: app:   build: ./app   networks:      - xxx-networknetworks: xxx-network:   external: true<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>解决方案3：<code>sudo service network-manager restart</code></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>k8s 日常笔记</title>
      <link href="k8s/"/>
      <url>k8s/</url>
      
        <content type="html"><![CDATA[<h1 id="k8s"><a href="#k8s" class="headerlink" title="k8s"></a>k8s</h1><h1 id="K8S学习笔记"><a href="#K8S学习笔记" class="headerlink" title="K8S学习笔记"></a>K8S学习笔记</h1><p>K8S ： 自动化运维管理docker程序</p><p>主从架构模型：Master 负责核心调度、管理、运维，Worker（上图的minions）节点则是执行用户程序 所有的Master Node 和Worker Node组成了K8S集群</p><h2 id="Master-Node"><a href="#Master-Node" class="headerlink" title="Master Node"></a>Master Node</h2><ul><li>API Server : k8s的请求入口服务</li><li>Scheduler : k8s所有的Worker Node的调度器</li><li>Controller Manager : k8s所有Worker Node的监控器。有很多具体的controller ， 例如Node Controller、Service Controller、Volume Controller 等。 Controller负责监控和调整worker node上部署的服务状态。</li><li>Etcd : k8s的存储服务。存储关键配置和用户配置，只能通过API Server才能读写数据。其他组件需要通过APIServer 才能读写数据</li></ul><h2 id="Worker-Node"><a href="#Worker-Node" class="headerlink" title="Worker Node"></a>Worker Node</h2><ul><li>Kubelet: Worker Node的监视器和Master Node的通讯器。定期向master汇报自己node上运行服务的状态，并接受来自master的指示</li><li>Kube-Proxy : 网络代理，负责Node在k8s里面的网络通讯和对外部网络流量的负载均衡</li><li>Container Runtime: Worker Node的运行环境（就是装好的docker运行环境）</li><li>Logging Layer : k8s的监控状态收集器（CPU、内存、磁盘、网络等）</li><li>Add-Ons : 管理插件的组件</li></ul><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h3><p>Pod 是可以在K8s中创建和管理的、最小的可部署的计算单元。就是Pod是K8s中一个服务的闭包，也就是一群可以共享网络、存储和计算资源的容器化服务的集合</p><p>举个例子： 同一个Pod之间的Container可以通过localhost来进行访问，并且可以挂载Pod内所有的数据卷，但是不同的Pod之间不行</p><p>img</p><p>k8s中的所有对象都通过yaml来表示。如下：</p><pre class="line-numbers language-none"><code class="language-none">apiVersion: v1kind: Pod #记录yaml的对象metadata: #记录pod自身的元数据name: memory-demonamespace: mem-examplespec: #记录了pod内部所有资源的详细信息containers: #记录了pod内的容器信息- name: memory-demo-ctr  image: polinux/stress  resources:    limits:      memory: "200Mi"    requests:      memory: "100Mi"  command: ["stress"] #容器入口命令  args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"] #入口参数  volumeMounts: #pod内的数据卷信息  - name: redis-storage    mountPath: /data/redisvolumes:- name: redis-storage  emptyDir: {}<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="Volume数据卷"><a href="#Volume数据卷" class="headerlink" title="Volume数据卷"></a>Volume数据卷</h3><p>数据卷Volume是Pod内部的磁盘资源，对应一个实体的数据卷，VolumeMounts只是Container的挂载点,对应的是Container的其中一个参数。但是，VolumeMounts依赖于Volume，只有当Pod内有Volume资源的时候，该Pod内部的container才可能有VolumeMount。</p><h3 id="Container容器"><a href="#Container容器" class="headerlink" title="Container容器"></a>Container容器</h3><p>一个Pod内可以有多个Container 容器分类有：</p><ul><li>标准容器 Application Container</li><li>初始化容器 Init Container</li><li>边车容器 Sidecar Container</li><li>临时容器 Ephemeral Container</li></ul><h3 id="Deployment和ReplicaSet"><a href="#Deployment和ReplicaSet" class="headerlink" title="Deployment和ReplicaSet"></a>Deployment和ReplicaSet</h3><p>Deployment的作用就是管理和控制Pod和ReplicaSet，管控他们运行在用户期望的状态下。也就是如果用户对Pod如果进行了更新，那么Deployment会先起一个ReplicaSet然后等到内部的Pod都处在Ready状态时，再把原本的ReplicaSet停掉，转移到新版本的ReplicaSet上。</p><p>ReplicaSet的作用就是管理和控制Pod</p><p>img</p><p>主要的区别可以看：<a href="https://blog.csdn.net/qq_41999455/article/details/104220882">https://blog.csdn.net/qq_41999455/article/details/104220882</a></p><h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><p>Service是K8s服务的核心，屏蔽了服务的细节，统一对外暴露服务接口。可以理解为是一个简单的服务注册发现组件。 Service类型：</p><ul><li>ClusterIP：最基本的类型，用作本集群内部的互相通信,也就是起一个转发的作用,避免一个服务宕机了可以保证访问到其他同级的服务</li><li>NodePort ： 集群每个节点上开发一个端口，对外提供服务，同时集群内的节点也可以通过这个内网节点IP 连接到这个端口。外部的话需要通过连接到对外IP的这个端口上。</li><li>Loadbalancer：这个是建立在NodePort服务之上的， 就是负载均衡器，需要有一个单独的IP地址，它会将请求通过这个IP地址分配给所有的外部节点IP，例如说通过round robin策略</li></ul><h3 id="ExternalName"><a href="#ExternalName" class="headerlink" title="ExternalName"></a>ExternalName</h3><p>最后是 ExternalName 服务，这个服务和前面的几种类型的服务有点分离。它创建一个内部服务，其端点指向一个 DNS 名。</p><p>举个例子， 我们在集群内部需要访问到一个python API这样的外部服务，一般而言我们直接通过对应的url 连接到外部服务商。 但是考虑到以后某个时候如果想把外部服务集成到集群中，还不希望去更改连接的地址（因为修改了url），这时候可以用ExternalName类型的服务了， 只要修改serviceType并设置正确标签就可以了。具体如下：</p><p>我们假设 pod-nginx 运行在 Kubernetes 集群中，但是 python api 服务在集群外部。</p><p>对应的 YAML 资源清单文件如下所示：</p><pre class="line-numbers language-none"><code class="language-none">kind: ServiceapiVersion: v1metadata:  name: service-pythonspec:  ports:  - port: 3000    protocol: TCP    targetPort: 443  type: ExternalName  externalName: remote.server.url.com<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>现在 <strong>pod-nginx</strong> 就可以很方便地通过 <code>http://service-python:3000</code> 进行通信了，就像使用 ClusterIP 服务一样，当我们决定将 python api 这个服务也迁移到我们 Kubernetes 集群中时，我们只需要将服务改为 ClusterIP 服务，并设置正确的标签即可，其他都不需要更改了。</p><p>Python api 仍然可以通过 <a href="http://service-python/">http://service-python</a> 访问</p><p>当我们创建一个 NodePort 的 Service 时，它也会创建一个 ClusterIP，而如果你创建一个 LoadBalancer，它就会创建一个 NodePort，然后创建一个 ClusterIP</p><p>此外我们还需要明白 Service 是指向 pods 的，Service 不是直接指向 Deployments 或 ReplicaSets，而是直接使用 labels 标签指向 Pod，这种方式就提供了极大的灵活性，因为通过什么方式创建的 Pod 其实并不重要。接下来我们通过一个简单的例子开始，我们用不同的 Service 类型来逐步扩展，看看这些 Service 是如何建立的。</p><p>Service主要负责K8sS集群内部的网络拓扑，Ingress则是负责集群外部访问集群内部的组件，是整个K8S集群的接入层，复杂集群内外通讯。</p><h3 id="Ingress"><a href="#Ingress" class="headerlink" title="Ingress"></a>Ingress</h3><p>Kubernetes Ingress 只是 Kubernetes 中的一个普通资源对象，需要一个对应的 Ingress Controller 来解析 Ingress 的规则，暴露服务到外部，比如 ingress-nginx，本质上来说它只是一个 Nginx Pod，然后将请求重定向到其他内部（ClusterIP）服务去，这个 Pod 本身也是通过 Kubernetes 服务暴露出去，最常见的方式是通过 LoadBalancer 来实现的。和Nginx相比它已经为我们做了所有的代理重定向工作，这为我们节省了大量的手动配置工作了。</p><h2 id="Istio"><a href="#Istio" class="headerlink" title="Istio"></a>Istio</h2><h3 id="Kubernetes-Services"><a href="#Kubernetes-Services" class="headerlink" title="Kubernetes Services"></a>Kubernetes Services</h3><p>我们可以用简短地说明下如何实现 Kubernetes Services，这这有助于理解 Istio 如何工作的。</p><p>img</p><p>图1: Kubernetes native service request</p><p>上图的 Kubernetes 集群中一共有两个节点和 4 个 pod，每个 pod 都有一个容器。服务 service-nginx 指向 nginx pods，服务 service-python 指向 python pods。红线显示了从 pod1-nginx 中的 nginx 容器向 service-python 服务发出的请求，该服务将请求重定向到 pod2-python。</p><p>默认情况下，ClusterIP 服务进行简单的随机或轮询转发请求，Kubernetes 中的 Services 并不存在于特定的节点上，而是存在于整个集群中。我们可以在下图 中看到更多细节:</p><p>img</p><p>图2: Kubernetes native service request with kube-proxy</p><p>上图要更详细点，Kubernetes 中的服务是由运行在每个节点上的 kube-proxy 组件实现的，该组件创建 iptables 规则，并将请求重定向到 Pod。因此，服务就是 iptables 规则。(还有其他不使用 iptables 的代理模式，但过程是相同的。)</p><p>现在我们来看一个配置了 Istio 的相同示例:</p><p>图3: Istio Control Plane programs istio-proxy</p><p>上图中可以看到集群中安装了 Istio，每个 pod 都有第二个称为 istio-proxy 的 sidecar 容器，该容器在创建期间会自动将其注入到 pods 中。</p><p>Istio 最常见的代理是 <strong>Envoy</strong>，当然也可以使用其他代理（如 Nginx），所以我们将代理称为 istio-proxy。</p><p>我们可以看到不再显示 kube-proxy 组件，这样做是为了保持图像的整洁，这些组件仍然存在，但是拥有 istio-proxy 的 pods 将不再使用 kube-proxy 组件了。</p><p>每当配置或服务发生变化时，Istio 控制平面就会对所有 istio-proxy sidecars 进行处理，类似于图 2 中 Kubernetes API 处理所有 kube-proxy 组件的方式。Istio 控制平面使用现有的 Kubernetes 服务来接收每个服务点所指向的所有 pods ，通过使用 pod IP 地址，Istio 实现了自己的路由。</p><p>在 Istio 控制平面对所有 istio-proxy sidecars 处理之后，它看起来是这样的:</p><p>img</p><p>图4: Istio Control Plane programmed all istio-proxys</p><p>在图 4 中，我们看到 Istio 控制平面如何将当前配置应用到集群中的所有 istio-proxy 容器，Istio 将把 Kubernetes 服务声明转换成它自己的路由声明。</p><p>让我们看看如何使用 Istio 发出请求:</p><p>图5: Request made with Istio</p><p>在上图中，所有的 istio-proxy 容器已经被 Istio 控制平面所管控，并包含所有必要的路由信息，如图 3/4 所示，来自 pod1-nginx 的 nginx 容器向 service-python 发出请求。</p><p>请求被 pod1-nginx 的 istio-proxy 容器拦截，并被重定向到一个 python pod 的 istio-proxy 容器，该容器随后将请求重定向到 python 容器。</p><h3 id="发生了什么？"><a href="#发生了什么？" class="headerlink" title="发生了什么？"></a>发生了什么？</h3><p>图 1-5 显示了使用 nginx 和 python pod 的 Kubernetes 应用程序的相同示例，我们已经看到了使用默认的 Kubernetes 服务和使用 Istio 是如何发生请求的。</p><p><strong>重要的是</strong>：无论使用什么方法，结果都是相同的，并且不需要更改应用程序本身，只需要更改基础结构代码。</p><p>为什么要使用 Istio?</p><p>如果在使用 Istio 的时候没有什么变化（nginx pod 仍然可以像以前一样连接到 python pod），那么我们为什么还要使用 Istio 呢？</p><p><strong>其惊人的优势是</strong>，现在所有流量都通过每个 Pod 中的 istio-proxy 容器进行路由，每当 istio-proxy 接收并重定向一个请求时，它还会将有关该请求的信息提交给 Istio 控制平面。因此 Istio 控制平面可以准确地知道该请求来自哪个 pod、存在哪些 HTTP 头、从一个istio-proxy 到另一个 istio-proxy 的请求需要多长时间等等。在具有彼此通信的服务的集群中，这可以提高可观察性并更好地控制所有流量。</p><p><strong>先进的路由</strong>，Kubernetes 内部 Services 只能对 pods 执行轮询或随机分发请求，使用 Istio 可以实现更复杂的方式。比如，如果发生错误，根据请求头进行重定向，或者重定向到最少使用的服务。</p><p><strong>部署</strong>，它允许将一定比例的流量路由到特定的服务版本，因此允许绿色/蓝色和金丝雀部署。</p><p><strong>加密</strong>，可以对 pods 之间从 istio-proxy 到 istio-proxy 的集群内部通信进行加密。</p><p><strong>监控/图形</strong>，Istio 可以连接到 Prometheus 等监控工具，也可以与 Kiali 一起展示所有的服务和他们的流量。</p><p>img</p><p><strong>追踪</strong>，因为 Istio 控制平面拥有大量关于请求的数据，所以可以使用 Jaeger 等工具跟踪和检查这些数据。</p><p>img</p><p><strong>多集群 mesh，</strong>Istio 有一个内部服务注册中心，它可以使用现有的 Kubernetes 服务，但是也可以从集群外部添加资源，甚至将不同的集群连接到一个网格中。</p><p>img</p><p><strong>Sidecar 注入</strong>，为了使 Istio 工作，每一个作为网状结构一部分的 pod 都需要注入一个 istio-proxy sidecar，这可以在 pod 创建期间为整个命名空间自动完成（也可以手动完成）。</p><h3 id="Istio-会取代-Kubernetes-的服务吗？"><a href="#Istio-会取代-Kubernetes-的服务吗？" class="headerlink" title="Istio 会取代 Kubernetes 的服务吗？"></a>Istio 会取代 Kubernetes 的服务吗？</h3><p>当然不会，当我开始使用 Istio 时，我问自己的一个问题是它是否会取代现有的 Kubernetes 服务，答案是否定的，因为 Istio 会使用现有的 Kubernetes 服务获取它们的所有 endpoints/pod IP 地址。</p><h3 id="Istio-取代了-Kubernetes-的-Ingress-吗？"><a href="#Istio-取代了-Kubernetes-的-Ingress-吗？" class="headerlink" title="Istio 取代了 Kubernetes 的 Ingress 吗？"></a>Istio 取代了 Kubernetes 的 Ingress 吗？</h3><p>是的，Istio 提供了新的 CRD 资源，比如 Gateway 和 VirtualService，甚至还附带了 ingress 转换器 istioctl convert-ingress，下图显示了 Istio 网关如何处理进入流量，网关本身也是一个 istio-proxy 组件。</p><p>img</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Istio 无疑在 Kubernetes 之上又增加了另一层次的复杂性，但是对于现代微服务架构来说，它实际上提供了一种比必须在应用程序代码本身中实现跟踪或可观察性更简单的方法。</p><p>关于 Istio 更多的使用说明，可以查看官方文档 ﻿<a href="https://istio.io/">https://istio.io</a> 了解更多。</p><h2 id="namespace"><a href="#namespace" class="headerlink" title="namespace"></a>namespace</h2><p>namespace是为了把一个k8s集群划分为若干个资源不可共享的虚拟集群而诞生的。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>后台日常笔记</title>
      <link href="hou-tai-ri-chang-bi-ji/"/>
      <url>hou-tai-ri-chang-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>k8s<br>sidecar ： istio-proxy </p><p>配置<br>docker</p><p>后台：整体概念： 处理什么事情 - &gt; 转发、路由（主流怎么用 ）-&gt;</p><h1 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h1><h2 id="后台主要解决的事情"><a href="#后台主要解决的事情" class="headerlink" title="后台主要解决的事情"></a>后台主要解决的事情</h2><ul><li><p>网络包</p></li><li><p>定时器</p></li><li><p>存储</p><h2 id="网络包"><a href="#网络包" class="headerlink" title="网络包"></a>网络包</h2><p>从包的生命期开始：来源、解析 、去向</p></li><li><p>来源</p><ul><li>上级服务是如何将包投递过来的</li><li>这个过程包的正确性</li><li>投递速度</li><li>健壮性如何保证（丢包、宕机）</li><li>恶意攻击</li><li>高负载情况</li></ul></li><li><p>解析</p><ul><li>效率</li><li>正确性</li><li>校验</li><li>完整性</li></ul></li><li><p>去向 </p><ul><li>包的下一级如何处理的</li><li>如何投递的</li><li>具体给谁</li><li>与下一级的关系</li><li>效率（线程、如果有锁的话粒度如何）</li></ul></li></ul><h2 id="定时器"><a href="#定时器" class="headerlink" title="定时器"></a>定时器</h2><p>主要关心：精度、追frame</p><h3 id="思考方式"><a href="#思考方式" class="headerlink" title="思考方式"></a>思考方式</h3><ol><li>需求背景</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>后台日常笔记</title>
      <link href="bian-yi-huan-jing-bu-shu/"/>
      <url>bian-yi-huan-jing-bu-shu/</url>
      
        <content type="html"><![CDATA[<h1 id="C-编译环境部署"><a href="#C-编译环境部署" class="headerlink" title="C++ 编译环境部署"></a>C++ 编译环境部署</h1>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>RabbitMQ 知识库</title>
      <link href="rabbitmq/"/>
      <url>rabbitmq/</url>
      
        <content type="html"><![CDATA[<h1 id="RabbitMq"><a href="#RabbitMq" class="headerlink" title="RabbitMq"></a>RabbitMq</h1><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><p>略咯~~ 网上很多的</p><h2 id="队列模式"><a href="#队列模式" class="headerlink" title="队列模式"></a>队列模式</h2><p>RabbitMq队列具备两种模式：default和lazy。在队列声明的时候可以通过x-queue-mode参数来设置队列的模式，取值为default和lazy。</p><p>RabbitMQ从3.6.0版本开始引入了惰性队列的概念，即将接受到的消息直接存入文件系统中，而在消费者消费到相应的消息时才会被加载到内存中，它的一个重要的设计目标是能够支持更长的队列，即支持更多的消息存储。惰性队列虽然减少了内存的消耗，但是增加了I/O的使用，因此对于持久化的消息，本身就不可避免磁盘I/O，使用惰性队列是较佳的选择。要注意的是，如果惰性队列中存储的是非持久化的消息，重启之后消息一样会丢失。</p><p> 默认情况下，消息会先存放在内存中，即使是持久化消息也会在内存中驻留一份备份，这部分是由Mq内部的存储结构决定的，后面有写~ 当mq需要释放内存的时候，会将内存中的消息换页（page）到磁盘中，这个操作比较耗时，也会阻塞队列的操作，无法接受新的消息，严重的话甚至会长达几分钟。 </p><p>官方说明文档： </p><p><a href="https://www.rabbitmq.com/lazy-queues.html">https://www.rabbitmq.com/lazy-queues.html</a></p><hr><p>惰性队列和普通队列相比，只有很小的内存开销。这里很难对每种情况给出一个具体的数值，但是我们可以类比一下：当发送1千万条消息，每条消息的大小为1KB，并且此时没有任何的消费者，那么普通队列会消耗1.2GB的内存，而惰性队列只消耗1.5MB的内存。</p><p>据官网测试数据显示，对于普通队列，如果要发送1千万条消息，需要耗费801秒，平均发送速度约为13000条/秒。如果使用惰性队列，那么发送同样多的消息时，耗时是421秒，平均发送速度约为24000条/秒。出现性能偏差的原因是普通队列会由于内存不足而不得不将消息换页至磁盘。如果有消费者消费时，惰性队列会耗费将近40MB的空间来发送消息，对于一个消费者的情况，平均的消费速度约为14000条/秒。</p><h2 id="镜像队列-vs-仲裁队列"><a href="#镜像队列-vs-仲裁队列" class="headerlink" title="镜像队列 vs 仲裁队列"></a>镜像队列 vs 仲裁队列</h2><p>以下围绕同步模型和性能。 </p><h3 id="镜像队列"><a href="#镜像队列" class="headerlink" title="镜像队列"></a>镜像队列</h3><p>RabbitMQ的集群在默认模式下，队列实例只存在于一个节点上，既不能保证该节点崩溃的情况下队列还可以继续运行，也不能线性扩展该队列的吞吐量。虽然RabbitMQ的队列实际只会在一个节点上，但元数据可以存在于各个节点上。举个例子来说，当创建一个新的交换器时，RabbitMQ会把该信息同步到所有节点上，这个时候客户端不管连接到哪个RabbitMQ节点，都可以访问到这个新的交换器，也就能找到交换器下的队列：</p><p><img src="https://gitee.com/skykens/photos/raw/master/OFqjExcshZpj=tsMaQJXIBApa06sCdts0=91EuBoQhpl31544076799852compressflag.png" alt="img"></p><p>RabbitMQ内部的元数据主要有：</p><ol><li>队列元数据：队列名称和属性</li><li>交换器元数据：交换器名称，类型和属性</li><li>绑定元数据：路由信息</li></ol><p>尽管交换器和绑定关系能够在单点故障问题上幸免于难，但是队列和其上存储的消息却不行，它们仅存在于单个节点上。引入镜像队列的机制，可以将队列镜像到集群中的其它Broker节点之上，如果集群中的一个节点失效了，队列能够自动地切换到镜像中的另一个节点上以保证服务的可用性。通常情况下，针对每一个配置镜像的队列都包含一个主拷贝和若干个从拷贝，相应架构如下：</p><p><img src="https://gitee.com/skykens/photos/raw/master/0JeBkrdwK7qUqTzySTsBegsoe6Bhg9PiMa3HYIMFSnPGQ1544076799852compressflag.png" alt="img"></p><p>除了发送消息外的所有动作都只会向主拷贝发送，然后再由主拷贝将命令执行的结果广播给各个从拷贝，从拷贝实际只是个冷备（默认的情况下所有RabbitMQ节点上都会有镜像队列的拷贝），如果使用消息确认模式，RabbitMQ会在主拷贝和从拷贝都安全的接受到消息时才通知生产者。从这个结构上来看，如果从拷贝的节点挂了，实际没有任何影响，如果主拷贝挂了，那么会有一个重新选举的过程，这也是镜像队列的优点，除非所有节点都挂了，才会导致消息丢失。重新选举后，RabbitMQ会给消费者一个消费者取消通知（Consumer Cancellation），让消费者重连新的主拷贝。</p><h4 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h4><p>不同于普通的非镜像队列，镜像队列的实现结构如下：<br><img src="https://gitee.com/skykens/photos/raw/master/8=93STeAbnCfJrE7VLgGxEOr1IDyCPEUR8Lb8jY1F6IiF1544076799852compressflag.png" alt="img"></p><p>所有对镜像队列主拷贝的操作，都会通过GM同步到各个slave节点，Coodinator负责组播结果的确认。GM是一种可靠的组播通信协议，该协议能够保证组播消息的原子性，即保证组内的存活节点要么都收到消息要么都收不到。</p><p>GM的组播并不是由master来负责通知所有slave的（目的是为了避免master压力过大，同时避免master失效导致消息无法最终ack)，RabbitMQ把所有节点组成一个链表，每个节点都会监控位于自己左右两边的节点，当有节点新增时，相邻的节点保证当前广播的消息会复制到新节点上；当有节点失效时，相邻的节点会接管以保证本次广播的消息会复制到所有的节点。操作命令由master发起，也由master最终确认通知到了所有的slave，而中间过程则由slave接力的方式进行消息传播。</p><p><img src="https://gitee.com/skykens/photos/raw/master/WUD5eU3KpIo3rCJZti5tw1TvD0yeQMZYCnHvQ6i5fXoCb1544076799853compressflag.png" alt="img"></p><p>镜像队列性能低于应有的速度，使用leader 队列 和一个以上的镜像队列，读写操作都经过leader队列，同时将所有的命令复制到镜像队列里，一旦所有的镜像队列都持有这个消息，leader队列才会发送confirm。 一旦过程中leader下线了，一个镜像队列将会成为leader使得整个队列依然可用。</p><p>在节点与节点之间有一个Inter-node Communication Buffer，用来临时缓存尚未同步的消息， 在某些情况下，节点间的通信量可能非常大，并且会耗尽缓存区的容量，默认配置是128M 。如果要修改可以使用环境变量。当缓存区耗尽的时候会导致节点阻塞。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token assign-left variable">RABBITMQ_DISTRIBUTION_BUFFER_SIZE</span><span class="token operator">=</span><span class="token number">192000</span>  <span class="token comment">#直接设置值</span><span class="token assign-left variable">RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS</span><span class="token operator">=</span><span class="token string">"+zdbbl 192000"</span> <span class="token comment">#增加值  单位： 千字节</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>当缓冲区处在满负载的情况下，节点会记录一个警告，提到一个负载严重的端口</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token number">2019</span>-04-06 <span class="token number">22</span>:48:19.031 <span class="token punctuation">[</span>warning<span class="token punctuation">]</span> <span class="token operator">&lt;</span><span class="token number">0.242</span>.<span class="token operator"><span class="token file-descriptor important">0</span>&gt;</span> rabbit_sysmon_handler busy_dist_port <span class="token operator">&lt;</span><span class="token number">0.1401</span>.<span class="token operator"><span class="token file-descriptor important">0</span>&gt;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>具体官方描述 ： <a href="https://www.rabbitmq.com/runtime.html#distribution-buffer">https://www.rabbitmq.com/runtime.html#distribution-buffer</a></p><p>问题1： 当broker重新上线的话需要面临一个问题： 是否同步镜像，如果同步了，原本队列中的消息就会丢失，同步意味着将当前消息从leader复制到镜像。</p><p>问题2： 接上个问题， 同步过程会阻塞整个队列，导致队列不可用。如果队列很短影响不大，producer可以重新发送那些在同步过程中被拒绝的消息。但是当队列很长的话，影响就很大了。</p><p>默认情况下会自动同步，如果选择不同步镜像的话：所有新消息都会得到复制，但是现有消息不会，也就是说冗余会减少，会存在更大的消息丢失的可能。同时滚动更新也会成为问题，因为重启broker会丢失所有数据需要同步来恢复数据冗余。</p><h3 id="仲裁队列"><a href="#仲裁队列" class="headerlink" title="仲裁队列"></a>仲裁队列</h3><p>基于raft 算法的实现，<strong>仲裁队列比镜像队列更安全，并且吞吐量更高</strong>。那么，这是什么意思呢？</p><p>每个仲裁队列是一个复制队列。它有一个Leader和多个Follower。具有5的复制因子的仲裁队列将由五个复制的队列组成：领导者和四个跟随者。每个复制的队列将托管在不同的节点上。</p><p>客户（发布者和消费者）始终与Leader 进行交互，Leader然后将所有命令（写入，读取，确认等）复制到跟随者。追随者根本不与客户互动。它们仅出于冗余目的而存在，从而在Broker发生故障，宕机时，另一个Broker上的跟随者副本将被选为Leader，并且服务将继续。</p><p>仲裁队列之所以有其名称，是因为所有操作（消息复制和领导者选举）都需要多数副本（称为仲裁）才能达成一致。发布者发送消息时，只有大多数副本将消息写入磁盘后，队列才能确认它。这意味着缓慢的少数群体不会降低整个队列的速度。同样，只有在多数人同意的情况下才能选举领导者，这可以防止两个领导者在发生网络分区时接受消息。因此，仲裁队列的重点是可用性的一致性。</p><p>仲裁队列的<strong>建议副本数</strong>是群集节点的仲裁数（但不少于三个）。</p><p>要声明仲裁队列，请将x-queue-type queue参数设置为quorum （默认值为classic）</p><h4 id="性能特点"><a href="#性能特点" class="headerlink" title="性能特点"></a>性能特点</h4><p>仲裁队列被设计为以延迟为代价来交换吞吐量，并且已经过测试，并与几种消息大小的3、5和7节点配置中的持久<a href="https://www.rabbitmq.com/ha.html">经典镜像队列</a>进行了比较。在同时使用使用者acks和发布者的情况下，<strong>确认已观察到仲裁队列与经典镜像队列具有相等或更大的吞吐量</strong>。</p><p>由于仲裁队列会在执行任何操作之前将所有数据持久保存到磁盘，因此建议使用尽可能快的磁盘。仲裁队列还受益于使用较高预取值的使用者，以确保在确认流经系统并允许消息及时传递的同时，不会使使用者感到饥饿。</p><p>由于仲裁队列的磁盘I / O繁忙特性，其吞吐量随着消息大小的增加而降低。</p><p>就像镜像队列一样，仲裁队列也受群集大小的影响。仲裁队列中的副本越多，通常其吞吐量就越低，因为必须做更多的工作来复制数据并达成共识。</p><p>具体可以看官方描述：<a href="https://www.rabbitmq.com/quorum-queues.html">https://www.rabbitmq.com/quorum-queues.html</a></p><h3 id="两种队列的区别"><a href="#两种队列的区别" class="headerlink" title="两种队列的区别"></a>两种队列的区别</h3><table><thead><tr><th align="left">特性</th><th align="left">镜像队列</th><th align="left">仲裁队列</th></tr></thead><tbody><tr><td align="left"><a href="https://www.rabbitmq.com/queues.html">非持久性队列</a></td><td align="left">yes</td><td align="left">no</td></tr><tr><td align="left"><a href="https://www.rabbitmq.com/queues.html">排他性</a></td><td align="left">yes</td><td align="left">no</td></tr><tr><td align="left">每条消息的持久性</td><td align="left">per message</td><td align="left">always</td></tr><tr><td align="left">成员变更</td><td align="left">automatic</td><td align="left">manual</td></tr><tr><td align="left"><a href="https://www.rabbitmq.com/ttl.html">Message TTL</a></td><td align="left">yes</td><td align="left">no</td></tr><tr><td align="left"><a href="https://www.rabbitmq.com/ttl.html#queue-ttl">Queue TTL</a></td><td align="left">yes</td><td align="left">yes</td></tr><tr><td align="left"><a href="https://www.rabbitmq.com/maxlength.html">Queue length limits</a></td><td align="left">yes</td><td align="left">yes (except x-overflow: reject-publish-dlx)</td></tr><tr><td align="left"><a href="https://www.rabbitmq.com/lazy-queues.html">Lazy队列</a></td><td align="left">yes</td><td align="left">partial (see <a href="https://www.rabbitmq.com/quorum-queues.html#memory-limit">Memory Limit</a>)</td></tr><tr><td align="left"><a href="https://www.rabbitmq.com/priority.html">优先级消息</a></td><td align="left">yes</td><td align="left">no</td></tr><tr><td align="left"><a href="https://www.rabbitmq.com/consumer-priority.html">消费者优先级</a></td><td align="left">yes</td><td align="left">yes</td></tr><tr><td align="left"><a href="https://www.rabbitmq.com/dlx.html">DLE</a></td><td align="left">yes</td><td align="left">yes</td></tr><tr><td align="left">Adheres to <a href="https://www.rabbitmq.com/parameters.html#policies">policies</a></td><td align="left">yes</td><td align="left">partial (dlx, queue length limits)</td></tr><tr><td align="left">对内存 <a href="https://www.rabbitmq.com/alarms.html">内存预警</a> 做出反应</td><td align="left">yes</td><td align="left">partial (truncates log)</td></tr><tr><td align="left">处理消费失败信息</td><td align="left">no</td><td align="left">yes</td></tr><tr><td align="left">全局的<a href="https://www.rabbitmq.com/quorum-queues.html#global-qos">QOS值</a></td><td align="left">yes</td><td align="left">no</td></tr></tbody></table><h2 id="DLE（Dead-letter-exchange-）"><a href="#DLE（Dead-letter-exchange-）" class="headerlink" title="DLE（Dead-letter-exchange ）"></a>DLE（Dead-letter-exchange ）</h2><p>一些消息在被broker接收的时候就无法被投递或者处理， 这些信息被称为dead message 。 </p><p>被认为dead message 的情况： </p><ul><li>不被消费者认可消息被否定确认，使用 <code>channel.basicNack</code> 或 <code>channel.basicReject</code> ，并且此时<code>requeue</code> 属性被设置为<code>false</code>。</li><li>TTL过期</li><li>队伍达到了最大容量</li></ul><h3 id="死信队列应用场景"><a href="#死信队列应用场景" class="headerlink" title="死信队列应用场景"></a>死信队列应用场景</h3><p>一般用在较为重要的业务队列中，确保未被正确消费的消息不被丢弃，一般发生消费异常可能原因主要有由于消息信息本身存在错误导致处理异常，处理过程中参数校验异常，或者因网络波动导致的查询异常等等，当发生异常时，当然不能每次通过日志来获取原消息，然后让运维帮忙重新投递消息。通过配置死信队列，可以让未正确处理的消息暂存到另一个队列中，待后续排查清楚问题后，编写相应的处理代码来处理死信消息，这样比手工恢复数据要好太多了。</p><h2 id="用AlternateExchange中收集不可路由的消息"><a href="#用AlternateExchange中收集不可路由的消息" class="headerlink" title="用AlternateExchange中收集不可路由的消息"></a>用AlternateExchange中收集不可路由的消息</h2><p><strong>不可路由消息会导致：</strong> </p><ul><li>返回到一个不断重新发送他们的损坏的应用程序</li><li>Mq被恶意活动攻击导致失去响应</li><li>关键数据丢失</li></ul><p>可以通过基于消息头的的<code>mandatory flag</code>来设置如何处理： <code>true</code> 则原路返回，<code>false</code> 则是静默删除，<code>log</code>可以记录返回的消息但是<code>log</code>无法提供无法访问的<code>exchange</code>或<code>queue</code> 的信息 </p><p>使用AlternateExchange来捕捉不可路由的消息，主交换器上的 <code>mandatory flag</code>需要set，备用交换器上不能，主交换器将消息转发到备用交换器上，备用交换器将消息发送给备用队列（ 他们之间的绑定方式是<code>fanot</code>），将不可路由消息交给专门的<code>consumer</code>处理吧。</p><p><strong>What happens when the mandatory flag is set with an alternate exchange?</strong> </p><p>There is still a chance that messages won’t be routed if an alternate exchange is provided. The service may be unreachable, or the alternate queue may not be specified correctly. You might accidentally specify a non-existent exchange as well.</p><p>当消息路由到备用交换的时候，RabbitMq将把消息标记为已交付。 </p><h2 id="HA方案"><a href="#HA方案" class="headerlink" title="HA方案"></a>HA方案</h2><p>默认情况下，queue可以认为是只存在于它被声明的那个节点中，但是broker和binding可以认为存在于集群中的所有节点中. 可以通过镜像的方式，将queue复制到其它的节点中，以此来提高可用性</p><ul><li>镜像队列之间彼此形成了一主多从的关系，当主镜像队列因为某些原因消失时，一个从镜像自动被推选为主镜像</li><li>不论客户端连接到哪个结点，它都将连接到主镜像队列中，所有队列的操作也都是通过主镜像队列来完成，这样就保证了队列的FIFO特性</li><li>发布到主镜像队列中的消息将会被自动镜像到所有的从镜像中</li><li>如果主镜像中的消息确认已经被消费了，那么从镜像会自动将该消息删除</li><li>这种镜像的方式并不能将流量分散到各个节点，因为每个节点做的事情是一样的，但是它提高了可用性，如果主镜像队列因为某些原因消失了，那么从镜像可以自动升级为主镜像，保证了队列的可用性</li></ul><h2 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h2><p>Rabbitmq队列是不存在于多个节点上的结构，假设有一个负载平衡的、 HA (高可用性) RabbitMQ 集群，如下所示:</p><p><img src="C:\Users\jimlu\Desktop\新建文件夹\rabbitmq-cluster-with-lb.png" alt="RabbitMQ Cluster with Load Balancer"></p><p>节点1-3在彼此之间进行复制，以便在每个节点之间同步所有符合 HA策略的队列的快照。假设我们登录到 RabbitMQ 管理控制台并创建一个新的 ha 配置的队列。我们的负载均衡器是以循环的方式配置的，在这个实例中，为了方便起见，我们被定向到 Node # 2。我们的新 Queue 是在 Node # 2上创建的。注意: 可以显式选择您希望 Queue 驻留的节点，但是为了本示例的目的，我们忽略这一点。</p><p>现在，我们的新 Queue“ NewQueue”存在于 Node # 2中。我们的 HA 策略开始生效，Queue 在所有节点上复制。我们开始向 Queue 添加消息，这些消息也在每个节点之间进行复制。本质上，是获取 Queue 的一个快照，并且在不确定的时间段过去之后在每个节点上复制该快照(当 Queue 的状态发生更改时，它实际上作为一个异步后台任务的发生)。</p><p>RabbitMQ 队列是一种单一结构。它只存在于创建它的节点上，与 HA 策略无关。队列总是它自己的主人，并且拥有0…n个从属队列组成。根据上面的示例，节点 # 2上的“ NewQueue”是 Master-Queue，因为这是创建 Queue 的节点。它包含2个从队列-它的对应节点 # 1和 # 3。让我们假设 Node # 2由于某种原因而死亡; 假设整个服务器都关闭了。以下是“ NewQueue”将会发生的情况。</p><ol><li> 节点 # 2不返回心跳，并且被认为是从集群上掉线了</li><li>主队列不再可用(它随节点 # 2一起死亡)</li><li>RabbitMQ 将 Node # 1或 # 3上的“ NewQueue”从属实例提升为 master</li></ol><p>这是 RabbitMQ 中的标准 HA 行为。现在让我们看一下缺省场景，其中所有3个节点都是活的并且运行良好，节点 # 2上的“ NewQueue”实例仍然是主节点。</p><ol><li> 我们连接到 RabbitMQ，目标是“ NewQueue”</li><li> 我们的负载均衡器基于轮循确定一个合适的 Node</li><li> 我们被定向到一个合适的节点(比如说，节点 # 3)</li><li> RabbitMQ 确定“ NewQueue”主节点在 Node # 2上</li><li> 我们成功地连接到“ NewQueue”的主实例</li></ol><p>尽管我们的队列在每个 HA 节点上复制，但是每个 Queue 只有一个可用的实例，并且它驻留在创建它的节点上，或者在失败的情况下，提升为 master 的实例。在这种情况下，RabbitMQ 可以方便地将我们路由到该节点:</p><p><img src="https://gitee.com/skykens/photos/raw/master/rabbitmq-cluster-extra-network-hop.png" alt="RabbitMQ Cluster Exhibiting Extra Network-hop"></p><p>糟糕的是，为了到达目标Queue所在的Node，我们需要额外的网络跳转。按照上面的例子中，有3个节点和一个均衡的负载均衡器，我们将在大约66% 的请求上产生额外的网络跳转。每三个请求中只有一个(假设在任何三个唯一请求的分组中，我们被定向到不同的节点)将导致我们的请求被定向到正确的节点。</p><p>为了确保每个请求都被路由到正确的节点，我们有两个选择:</p><ol><li> 显式连接到目标 Queue 所在的节点</li><li> 在节点之间尽可能均匀地分布队列</li></ol><p>这两种解决方案都会立即引发问题。在第一个选择中，客户机应用程序必须知道 RabbitMQ 集群中的所有节点，并且还必须知道每个主队列驻留在哪里。如果一个 Node 宕机了，我们的应用程序怎么知道？更不用说这种设计打破了单一责任原则，提高了应用程序中的耦合级别。</p><p>第二个解决方案提供了一种设计，其中队列不链接到单个节点。基于我们的“ NewQueue”示例，我们不会简单地在单个节点上实例化一个新的 Queue。相反，在一个3节点的场景中，我们可以实例化3个队列; “ NewQueue1”、“ NewQueue2”和“ NewQueue3”，其中每个队列在一个单独的节点上实例化。</p><p> 例如，我们的客户端应用程序现在可以实现一个简单的随机化函数，选择上面的队列之一并显式地连接到它。在网络应用程序中，给定3个单独的 HTTP 请求，每个请求将针对上面的队列中的一个，并且没有队列会在所有3个请求中多于一次。现在，我们已经在集群中实现了合理的负载均衡，而没有使用传统的负载均衡器</p><p><img src="https://gitee.com/skykens/photos/raw/master/rabbitmq-cluster-with-randomiser.png" alt="RabbitMQ Cluster with Randomiser"></p><p> 但是我们仍然面临同样的问题; 我们的客户端应用程序需要知道队列驻留在哪里。所以让我们进一步研究解决方案，这样我们就可以避免这个缺点</p><p>首先，我们需要提供描述 RabbitMQ 基础结构的映射元数据。具体地说，队列所在的位置。这应该是弹性数据源，如数据库或缓存，而不是平面文件，因为多个数据源(至少是2个)可以并发地访问这些数据。</p><p>现在引入一个总是在线的服务，轮询 RabbitMQ，确定节点是否是活的。新队列也应该在这个服务中注册，它应该保持一个最新的注册表，提供关于节点及其队列的元数据:</p><p><img src="https://gitee.com/skykens/photos/raw/master/rabbitmq-cluster-with-monitor-service.png" alt="RabbitMQ Cluster with Monitor Service"></p><p> 在初始加载时，我们的客户端应用程序应该轮询这个服务并检索 RabbitMQ 元数据，然后为传入的请求保留这些元数据。如果请求由于节点受损而失败，客户机应用程序可以轮询队列元数据存储，返回最新的 RabbitMQ 元数据，并将消息重新路由到工作节点。</p><p>参考文章： </p><p><a href="https://insidethecpu.com/2014/11/17/load-balancing-a-rabbitmq-cluster/comment-page-1/">https://insidethecpu.com/2014/11/17/load-balancing-a-rabbitmq-cluster/comment-page-1/</a></p><p><a href="https://liqul.github.io/blog/rabbitmq-load-balancing/">https://liqul.github.io/blog/rabbitmq-load-balancing/</a></p><p><a href="https://honeypps.com/mq/rabbitmq-load-balance-3-keepalived-haproxy/">https://honeypps.com/mq/rabbitmq-load-balance-3-keepalived-haproxy/</a></p><h2 id="联邦机制"><a href="#联邦机制" class="headerlink" title="联邦机制"></a>联邦机制</h2><p>联邦机制的实现，依赖于RabbitMQ的Federation插件，该插件的主要目标是为了RabbitMQ可以在多个 Broker节点或者集群中进行消息的无缝传递。下面先假设一种场景，BrokerA服务部署在上海，BrokerB服务部署在北京。来自上海的ClientA向BrokerA的exchangeA发送消息网络延迟很小，但是北京的ClientB向BrokerA的exchangeA发送消息那么将会面临网络延迟的问题。Federation机制则可以帮助我们解决这个问题。</p><p>首先在BrokerA的exchangeA上与北京的BrokerB建立一条单向的Federation Link。此时Federation插件会在BrokerB上建立一个同名的交换器(可以配置，默认同名)，并且还会建立一个内部交换器federation:exchangeA-&gt;Broker B(其中Broker为集群名称)通过相同的绑定建进行绑定，于此同时Federation插件会建立一个federation:exchangeA-&gt;Broker B(BrokerB为集群名称)，并且将内部交换器federation:exchangeA-&gt;Broker B绑定到该队列。</p><p>Federation插件会在队列federation:exchangeA-&gt;Broker B与BrokerA中的交换器exchangeA之间建立一条AMQP连接来实时地消费队列federation:exchangeA-&gt;Broker B中的数据。这些操作都是内部的，对外部业务客户端来说这条Federation link建立在BrokerA的exchangeA和BrokerB的exchangeA之间。</p><p>此时ClientB可以以较小的网络延迟向BrokerB的exchangeA发送消息，并且该消息会被正确路由到BrokerA中的exchangeA中，通过Federation插件我们可以以较小的网络延迟向与客户端属于不同地域的Broker节点发送消息。</p><p>“max_hops=1”表示一条消息最多被转发的次数为1。</p><p>默认的交换器(每个vhost下都会默认创建一个名为””的交换器)和内部交换器，不能对其使用Federation的功能。</p><h3 id="联邦队列"><a href="#联邦队列" class="headerlink" title="联邦队列"></a>联邦队列</h3><p><img src="https://gitee.com/skykens/photos/raw/master/t01z1sz06a.jpeg"></p><p>队列queue1和queue2原本在broker2中，由于某种需求将其配置为federated queue并将broker1作为upstream。Federation插件会在broker1上创建同名的队列queue1和queue2，与broker2中的队列queue1和queue2分别建立两条单向独立的Federation link。当有消费者ClientA连接broker2并通过Basic.Consume消费队列queue1(或queue2)中的消息时，如果队列queue1(或queue2)中本身有若干消息堆积，那么ClientA直接消费这些消息，此时broker2中的queue1(或queue2)并不会拉取broker1中的queue1(或queue2)的消息；如果队列queue1(或queue2)中没有消息堆积或者消息被消费完了，那么它会通过Federation link拉取在broker1中的上游队列queue1(或queue2)中的消息(如果有消息)，然后存储到本地，之后再被消费者ClientA进行消费。</p><p>和federated exchange不同，一条消息可以在联邦队列间转发无限次。两个队列可以互为联邦队列。</p><p>如果两个队列互为联邦队列，队列中的消息除了被消费，还会转向有多余消费能力的一方，如果这种”多余的消费能力”在broker1和broker2中来回切换，那么消费也会在broker1和broker2中的队列queue中来回转发</p><p>federation queue只能使用Basic.Consume进行消费，并且不具备传递性。</p><p>参考文档： </p><p><a href="https://cloud.tencent.com/developer/article/1469331">https://cloud.tencent.com/developer/article/1469331</a></p><h2 id="数据读写过程"><a href="#数据读写过程" class="headerlink" title="数据读写过程"></a>数据读写过程</h2><h3 id="存储原理"><a href="#存储原理" class="headerlink" title="存储原理"></a>存储原理</h3><p>首先确认一个点，持久化和非持久化的消息都会落地磁盘，区别在于持久化的消息一定会写入磁盘(并且如果可以在内存中也会有一份)，而非持久化的消息只有在内存吃紧的时候落地磁盘。两种类型消息的落盘都是在RabbitMQ的持久层中完成的。</p><p>RabbitMQ的持久层只是一个逻辑上的概念，实际包含两个部分：</p><ul><li>队列索引(rabbit_queue_index)：负责维护队列中落盘消息的信息，包括消息的存储地点、是否己被交付给消费者、是否己被消费者ack等。 每个队列都有与之对应的一个rabbit_queue_index</li><li>消息存储(rabbit_msg_store)：以键值对的形式存储消息，它被所有vhost中的队列共享，在每个vhost中有且只有一个。rabbit_msg_store具体还可以分为 msg_store_persistent和msg_store_transient，msg_store_persistent负责持久化消息的持久化，重启后消息不会丢失；msg_store_transient负责 非持久化消息的持久化，重启后消息会丢失。</li></ul><p>消息(包括消息体、属性和headers)可以直接存储在rabbit_queue_index中，也可以被保存在rabbit_msg_store中。</p><p>最佳的配备方式是较小的消息存储在rabbit_queue_index中而较大的信息则存储在rabbit_msg_store中。消息大小的参数可以通过queue_index_embed_mgs_below来配置，默认大小4096，单位B。</p><p>rabbit_queue_index中以顺序的段文件来开始存储，后缀为”.idx”，每个段文件中包含固定的SEGMENT_ENTRY_COUNT条记录，SEGMENT_ENTRY_COUNT默认值是16384。</p><p>经过rabbit_msg_store处理的所有消息都会以追加的方式写入到文件中，当一个文件的大小超过指定的限制(filesizelimit)后，关闭这个文件再创建一个新的文件以供新的消息写入。文件名(文件后缀是”.rdq”)从0开始进行累加，因此文件名最小的文件也是最老的文件。在进行消息的存储时，RabbitMQ会在ETS(Erlang Term Storage)表中记录消息在文件中的位置映射(Index)和文件的相关信息(FileSummary)。</p><p>在读取消息的时候，先根据消息的ID(msg id)找到对应存储的文件，如果文件存在并且未被锁住，则直接打开文件，从指定位置读取消息的内容。如果文件不存在或者被锁住了，则发送请求由rabbit_msg_store进行处理。</p><p>消息删除是只是删除ETS表中该消息的相关信息，同时更新消息对应的存储文件的相关信息。执行消息删除操作时，并不立即对文件中的消息进行删除，也就是说消息依然在文件中，仅仅是被标识为垃圾数据而已。一个文件中都是垃圾数据时可以将这个文件删除。当检测到前后两个文件中的有效数据可以合并在一个文件中，并且所有的垃圾数据的大小和所有文件(至少有3个文件存在的情况下)的数据大小的比值超过设置的阀值GARBAGE FRACTION(默认值为0.5)时才会触发垃圾回收将两个文件合并。</p><h3 id="队列结构"><a href="#队列结构" class="headerlink" title="队列结构"></a>队列结构</h3><p>通常队列由rabbit_amqpqueue_process和backing_queue两部分组成：</p><ul><li>rabbit_amqpqueue_process：负责协议相关的消息处理(即接收生产者发布的消息、向消费者交付消息、处理消息的确认(包括生产端的confirm和消费端的ack))等</li><li>backing_queue：消息存储的具体形式和引擎，并向rabbit_amqpqueue_process提供接口以供调用</li></ul><p>如果消息发送的队列是空的且队列有消费者，该消息不会经过该队列直接发往消费者，如果无法直接被消费，则需要将消息暂存入队列，以便重新投递。消息在存入队列后，主要有以下几种状态：</p><ul><li>alpha：消息内容(包括消息体、属性和headers)和消息索引都存在内存中</li><li>beta：消息内容保存在磁盘中，消息索引都存在内存中</li><li>gamma：消息内容保存在磁盘中，消息索引在磁盘和内存中都存在</li><li>delta：消息内容和消息索引都在磁盘中</li></ul><p>持久化的消息，消息内容和消息索引必须都保存在磁盘中，才会处于上面状态中的一种，gamma状态只有持久化的消息才有这种状态。</p><p>对于没有设置优先级和镜像的队列来说，backing_queue的默认实现是rabbit_variable_queue，其内部通过5个子队列来体现消息的各个状态：</p><ul><li>Q1：只包含alpha状态的消息</li><li>Q2：包含beta和gamma的消息</li><li>Delta：包含delta的消息</li><li>Q3：包含beta和gamma的消息</li><li>Q4：只包含alpha状态的消息</li></ul><p>消息的状态一般变更方向是Q1-&gt;Q2-&gt;Delta-&gt;Q3-&gt;Q4，大体是从内存到磁盘然后再到内存中。</p><p><img src="C:\Users\jimlu\Desktop\新建文件夹\rabbitmq_struct.png" alt="rabbit队列结构"></p><p>消费者消费消息也会引起消息状态的转换。</p><ol><li>消费者消费时先从Q4获取消息，如果获取成功则返回。</li><li>如果Q4为空，则从Q3中获取消息，首先判断Q3是否为空，如果为空返回队列为空，即此时队列中无消息</li><li>如果Q3不为空，取出Q3的消息，然后判断Q3和Delta中的长度，如果都为空，那么Q2、Delta、Q3、Q4都为空，直接将Q1中的消息转移至Q4，下次直接从Q4中读取消息</li><li>如果Q3为空，Delta不为空，则将Delta中的消息转移至Q3中，下次直接从Q3中读取。</li><li>在将消息从Delta转移至Q3的过程中，是按照索引分段读取，首先读取某一段，然后判断读取的消息个数和Delta消息的个数，如果相等，判定Delta已无消息，直接将读取 Q2和读取到消息一并放入Q3，如果不相等，仅将此次读取的消息转移到Q3。</li></ol><p>通常在负载正常时，如果消息被消费的速度不小于接收新消息的速度，对于不需要保证可靠不丢失的消息来说，极有可能只会处于alpha状态。对于durable属性设置为true的消息，它一定会进入gamma状态，并且在开启publisher confirm机制时，只有到了gamma状态时才会确认该消息己被接收，若消息消费速度足够快、内存也充足，这些消息也不会继续走到下一个状态。</p><p>这里以持久化消息为例（可以看到非持久化消息的生命周期会简单很多），从Q1到Q4，消息实际经历了一个<code>RAM-&gt;DISK-&gt;RAM</code>这样的过程，<br>BackingQueue的设计有点类似于Linux的虚拟内存<code>Swap</code>区，</p><ul><li>当队列<code>负载很高</code>时，通过将部分消息放到磁盘上来<code>·</code>节省内存空间`，</li><li>当<code>负载降低</code>时，消息又从磁盘回到内存中，让整个队列有很好的<code>弹性</code>。<br>因此触发消息流动的主要因素是：</li></ul><ol><li><code>消息被消费</code>；</li><li><code>内存不足</code>。</li></ol><ul><li>RabbitMQ会根据<code>消息的传输速度</code>来计算当前<code>内存中允许保存的最大消息数量</code>（Traget_RAM_Count），</li><li>当<code>内存中保存的消息数量 + 等待ACK的消息数量 &gt; Target_RAM_Count</code>时，RabbitMQ才会把消息<code>写到磁盘</code>上，</li><li>所以说虽然理论上消息会按照<code>Q1-&gt;Q2-&gt;Delta-&gt;Q3-&gt;Q4</code>的顺序流动，但是并不是每条消息都会经历所有的子队列以及对应的生命周期。</li><li>从RabbitMQ的Backing Queue结构来看，当<code>内存不足</code>时，消息要经历多个生命周期，在Disk和RAM之间置换，这实际会<code>降低RabbitMQ的处理性能</code>（后续的流控就是关联的解决方法）。</li><li>对于持久化消息，RabbitMQ先将消息的内容和索引保存在磁盘中，然后才处于上面的某种状态（即只可能处于<code>alpha、gamma、delta</code>三种状态之一）。</li></ul><h3 id="消息什么时候会刷到磁盘？"><a href="#消息什么时候会刷到磁盘？" class="headerlink" title="消息什么时候会刷到磁盘？"></a>消息什么时候会刷到磁盘？</h3><ul><li>写入文件前会有一个<code>Buffer</code>，大小为1M（1048576），数据在写入文件时，首先会写入到这个Buffer，如果Buffer已满，则会将Buffer写入到文件（未必刷到磁盘）；</li><li>有个<code>固定的刷盘时间</code>：<code>25ms</code>，也就是不管Buffer满不满，每隔25ms，Buffer里的数据及未刷新到磁盘的文件内容必定会刷到磁盘；</li><li>每次消息写入后，如果没有后续写入请求，则会直接将已写入的消息刷到磁盘：使用Erlang的<code>receive x after 0</code>来实现，只要进程的信箱里没有消息，则产生一个timeout消息，而timeout会触发刷盘操作。</li></ul><h3 id="消息文件何时删除？"><a href="#消息文件何时删除？" class="headerlink" title="消息文件何时删除？"></a>消息文件何时删除？</h3><ul><li>当所有文件中的垃圾消息（已经被删除的消息）比例大于阈值（<code>GARBAGE_FRACTION = 0.5</code>）时，会触发<code>文件合并</code>操作（至少有三个文件存在的情况下），以提高磁盘利用率。</li><li><code>publish</code>消息时写入内容，<code>ack</code>消息时删除内容（更新该文件的有用数据大小），当一个文件的<code>有用数据等于0时</code>，删除该文件。</li></ul><p>参考文档： </p><p><a href="http://geosmart.github.io/2019/11/11/RabbitMQ%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99%E8%BF%87%E7%A8%8B/">http://geosmart.github.io/2019/11/11/RabbitMQ%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99%E8%BF%87%E7%A8%8B/</a></p><p><a href="http://geosmart.github.io/2019/11/11/RabbitMQ%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99%E8%BF%87%E7%A8%8B/">http://geosmart.github.io/2019/11/11/RabbitMQ%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99%E8%BF%87%E7%A8%8B/</a></p><p><a href="https://cloud.tencent.com/developer/article/1469333">https://cloud.tencent.com/developer/article/1469333</a></p><h2 id="内存及磁盘告警"><a href="#内存及磁盘告警" class="headerlink" title="内存及磁盘告警"></a>内存及磁盘告警</h2><p>当内存使用超过配置的阈值或者磁盘剩余空间低于配置的阈值时，RabbitMQ会暂时阻塞客户端的连接并停止接收从客户端发来的消息。被阻塞的Connection的状态要么是blocking，要么是blocked，前者对应于并不试图发送消息的Connection，后者对应于一直有消息发送的Connection，这种状态下的Connection会被停止发送消息。注意在一个集群中，如果一个Broker节点的内存或者磁盘受限，都会引起整个集群中所有的Connection被阻塞。</p><h3 id="内存告警"><a href="#内存告警" class="headerlink" title="内存告警"></a>内存告警</h3><p>默认情况下内存阈值为0.4，表示当RabbitMQ使用的内存超过40%时，会产生内存告警并阻塞所有生产者的连接。一旦告警被解除（有消息被消费或者从内存转储到磁盘等情况的发生），一切都会恢复正常。</p><p>在某个Broker快达到内存阈值时，会先尝试将队列中的消息换页到磁盘以释放内存空间。默认情况下，在内存到达内存阈值的50%时会进行换页动作。</p><h3 id="磁盘告警"><a href="#磁盘告警" class="headerlink" title="磁盘告警"></a>磁盘告警</h3><p>当剩余磁盘空间低于确定的阈值时，RabbitMQ同样会阻塞生产者，这样可以避免因非持久化的消息持续换页而耗尽磁盘空间导致服务崩溃。默认情况下，磁盘阈值为50MB。RabbitMQ会定期检测磁盘剩余空间，检测的频率与上一次执行检测到的磁盘剩余空间大小有关，随着磁盘剩余空间与磁盘阈值的接近，检测频率会有所增加。</p><h3 id="流控"><a href="#流控" class="headerlink" title="流控"></a>流控</h3><ul><li>当RabbitMQ出现内存(默认是0.4)或者磁盘资源达到阈值时，会触发流控机制：<code>阻塞Producer的Connection</code>，让生产者不能继续发送消息，直到内存或者磁盘资源得到释放。</li><li>Erlang进程之间并不共享内存（binaries类型除外），而是通过消息传递来通信，每个进程都有自己的进程邮箱。Erlang默认没有对进程邮箱大小设限制，所以当有大量消息持续发往某个进程时，会导致该进程邮箱过大，最终内存溢出并崩溃。</li><li>在RabbitMQ中，如果生产者持续高速发送，而消费者消费速度较低时，如果没有流控，很快就会使内部进程邮箱大小达到内存阈值，阻塞生产者（得益于block机制，并不会崩溃）。然后RabbitMQ会进行page操作，将内存中的数据持久化到磁盘中。</li><li>因此，要保证各个进程占用的内容在一个合理的范围，RabbitMQ的流控采用了一种信用机制(Credit)，为每个进程维护了4类键值对：<ul><li><code>{credit_from,From}</code>-该值表示还能向消息接收进程From<code>发送</code>多少条消息;</li><li><code>{credit_to,To}</code>-表示当前进程再<code>接收</code>多少条消息，就要向消息<code>发送</code>进程增加Credit数量;</li><li><code>credit_blocked</code>-表示当前进程被哪些进程block了，比如进程A向B发送消息，那么当A的进程字典中{credit_from,B}的值为0是，那么A的credit_blocked值为[B];</li><li><code>credit_deferred</code>-<code>消息接收</code>进程向<code>消息发送</code>进程增加Credit的<code>消息列表</code>，当进程被Block时会记录消息信息，<code>Unblock后依次发送这些消息</code>;</li></ul></li></ul><p><img src="https://gitee.com/skykens/photos/raw/master/rabbitmq_credit.png" alt="信用机制"></p><p>如图所示:</p><ul><li>A进程当前可以发送给B的消息有100条，每发一次，值减1，直到为0，A才会被Block住。</li><li>B消费消息后，会给A增加新的Credit，这样A才可以持续的发送消息。</li></ul><p>这里只画了两个进程，多进程串联的情况下，这中影响也就是从底向上传递的。</p><h2 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h2><ul><li>避免队列过长</li><li>不要重复打开和关闭连接或者通道</li><li>不要在线程间共享通道</li><li>不要用太多的链接或者通道</li><li>prefetch值设置要合理</li><li>不要忽略lazy queue</li><li>可以用TTL或者最大长度限制队列大小：如果吞吐量是优先级的话，可以通过从队列头部丢弃消息来保持队短</li><li>在底层节点上使用与核心数量一样多的队列（队列是单线程的）</li><li>持久化消息和队列</li><li>在不同内核上分割队列<ul><li>队列性能仅 限于一个 CPU 核心。因此，如果您将队列拆分到不同的核心，并且如果您有一个 RabbitMQ 集群，那么您将获得更好的性能。</li></ul></li></ul><h2 id="通过监控查看MQ线程负载"><a href="#通过监控查看MQ线程负载" class="headerlink" title="通过监控查看MQ线程负载"></a>通过监控查看MQ线程负载</h2> <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">rabbitmq-diagnostics observer<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="插件"><a href="#插件" class="headerlink" title="插件"></a>插件</h2><p>下面这个插件我认为可能有点用处</p><h3 id="consistent-hash-exchange"><a href="#consistent-hash-exchange" class="headerlink" title="consistent-hash-exchange"></a>consistent-hash-exchange</h3><p>当一个队列被绑定到这个交换机上，它会更具他的绑定权重在一致哈希环上分配一个或者多个分区。 对于每个属性hash（例如routing_key），会被放置到相应的散列环分区。该分区对应于一个绑定队列，消息被路由到该队列。</p><p>假设publish的消息的routing_key是合理均匀的，那么被路由的消息应该均匀的分布在所有的环分区上，从而根据他们的绑定权重排队。 </p><h2 id="Q1-Rabbitmq集群的负载均衡是怎么做的-？"><a href="#Q1-Rabbitmq集群的负载均衡是怎么做的-？" class="headerlink" title="Q1 Rabbitmq集群的负载均衡是怎么做的 ？"></a>Q1 Rabbitmq集群的负载均衡是怎么做的 ？</h2><p>A： Rabbitmq服务端本身是不支持负载均衡的，对于RabbitMQ集群来说，主要有两类负载均衡，客户端内部的和服务端的，客户端内部主要是采用负载均衡算法，服务端主要是采用代理服务器。服务端也可以用Haproxy作为负载均衡器。假设一个cluster里有两个实例，记作rabbitA和rabbitB。如果某个队列在rabbitA上创建，随后在rabbitB上镜像备份，那么rabbitA上的队列称为该队列的<strong>主队列</strong>（master queue），其它备份均为<strong>从队列</strong>。接下来，无论client访问rabbitA或rabbitB，最终消费的队列都是主队列。换句话说，即使在连接时主动连接rabbitB，RabbitMQ的cluster会自动把连接转向rabbitA。当且仅当rabbitA服务down掉以后，在剩余的从队列中再选举一个作为继任的主队列。出于这种机制而言， 负载均衡就不能简单的用随机化连接可以做到了</p><p>如果这种机制是真的，那么负载均衡就不能简单随机化连接就能做到了。需要满足下面的条件：</p><ol><li>队列本身的建立需要随机化，即将队列分布于各个服务器；</li><li>client访问需要知道每个队列的主队列保存在哪个服务器；</li><li>如果某个服务器down了，需要知道哪个从队列被选择成为继任的主队列。<br>于是，Load Balancing a RabbitMQ Cluster的作者给出了下图的结构。</li></ol><p>[<img src="rabbitmq-cluster-with-monitor-service.png" alt="负载均衡架构"></p><p>这还是颇有点复杂的。首先，在建立一个新队列的时候，Randomiser会随机选择一个服务器，这样能够保证队列均匀分散在各个服务器（这里暂且不考虑负载）。建立队列后需要在Meta data里记录这个队列对应的服务器；另外，Monitor Service是关键，它用于处理某个服务器down掉的情况。一旦发生down机，它需要为之前主队列在该服务器的队列重新建立起与服务器的映射关系。</p><p>这里会遇到一个问题，即怎么判断某个队列的主队列呢？一个方法是通过rabbitmqctl，如下面的例子：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">./rabbitmqctl -p production list_queues pid slave_pidsregistration-email-queue        <span class="token operator">&lt;</span>rabbit@mq01.2.1076.<span class="token operator"><span class="token file-descriptor important">0</span>&gt;</span>       <span class="token punctuation">[</span><span class="token operator">&lt;</span>rabbit@mq00.1.285.<span class="token operator"><span class="token file-descriptor important">0</span>&gt;</span><span class="token punctuation">]</span>registration-sms-queue  <span class="token operator">&lt;</span>rabbit@mq01.2.1067.<span class="token operator"><span class="token file-descriptor important">0</span>&gt;</span>       <span class="token punctuation">[</span><span class="token operator">&lt;</span>rabbit@mq00.1.281.<span class="token operator"><span class="token file-descriptor important">0</span>&gt;</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>可以看到pid和slave_pids分别对应主队列所在的服务器和从服务器（可能有多个）。利用这个命令就可以了解每个队列所在的主服务器了。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Shell脚本编写笔记</title>
      <link href="shell-jiao-ben-bian-xie-bi-ji/"/>
      <url>shell-jiao-ben-bian-xie-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="Shell脚本编写笔记"><a href="#Shell脚本编写笔记" class="headerlink" title="Shell脚本编写笔记"></a>Shell脚本编写笔记</h1><h2 id="读取外部输入"><a href="#读取外部输入" class="headerlink" title="读取外部输入"></a>读取外部输入</h2><pre class="line-numbers language-jsx" data-language="jsx"><code class="language-jsx">echo <span class="token operator">-</span>n <span class="token string">"enter:"</span> ##<span class="token operator">-</span>n <span class="token operator">:</span>不换行read 存放的变量<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="控制流-if-while-for"><a href="#控制流-if-while-for" class="headerlink" title="控制流 if / while / for"></a>控制流 if / while / for</h2><p>读取文件内容</p><pre class="line-numbers language-jsx" data-language="jsx"><code class="language-jsx"><span class="token keyword">while</span> read line<span class="token keyword">do</span>   body ##通常对变量line进行处理done <span class="token operator">&lt;</span> 文件名<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>循环n次</p><pre class="line-numbers language-jsx" data-language="jsx"><code class="language-jsx"><span class="token keyword">while</span> <span class="token punctuation">[</span> $i <span class="token operator">-</span>lt $num<span class="token punctuation">]</span><span class="token keyword">do</span>  bodydone<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>判断文件夹不存在的情况：</p><pre class="line-numbers language-jsx" data-language="jsx"><code class="language-jsx"><span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token operator">!</span> <span class="token operator">-</span>d <span class="token string">"文件夹"</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> then bodyfi<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="变量使用"><a href="#变量使用" class="headerlink" title="变量使用"></a>变量使用</h3><pre class="line-numbers language-jsx" data-language="jsx"><code class="language-jsx">$变量名<span class="token keyword">var</span><span class="token operator">=</span><span class="token function">$</span><span class="token punctuation">(</span>command<span class="token punctuation">)</span> #变量接收命令值<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>在echo命令中使用变量需要用双引号：</p><pre class="line-numbers language-jsx" data-language="jsx"><code class="language-jsx">echo <span class="token string">"print my $变量"</span>echo <span class="token string">"print my ${变量}"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>在shell中，会对双引号里面的内容进行转义，但是对单引号的不会</p><h2 id="变量运算"><a href="#变量运算" class="headerlink" title="变量运算"></a>变量运算</h2><pre class="line-numbers language-jsx" data-language="jsx"><code class="language-jsx"><span class="token keyword">let</span> <span class="token string">'++i'</span> <span class="token keyword">let</span> <span class="token string">'--i'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="函数编写"><a href="#函数编写" class="headerlink" title="函数编写"></a>函数编写</h2><pre class="line-numbers language-jsx" data-language="jsx"><code class="language-jsx"><span class="token keyword">function</span> <span class="token function">func</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>body<span class="token punctuation">}</span>##参数我一般直接用全局变量## 返回值用echo $<span class="token operator">?</span> 获取<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Awk"><a href="#Awk" class="headerlink" title="Awk"></a>Awk</h2><h1 id="Sed"><a href="#Sed" class="headerlink" title="Sed"></a>Sed</h1>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
